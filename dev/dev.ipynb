{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from src.VLLMServer import launch_command,get_client_dict,ask_llm,ask_llm_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export CUDA_VISIBLE_DEVICES=1\n",
      "python -m vllm.entrypoints.openai.api_server --model hatakeyama-llm-team/Tanuki-8B-Instruct --max-model-len 4000 --port 8000 --gpu-memory-utilization 0.2 \n",
      "\n",
      "export CUDA_VISIBLE_DEVICES=1\n",
      "python -m vllm.entrypoints.openai.api_server --model microsoft/Phi-3-medium-128k-instruct --max-model-len 4000 --port 8001 --gpu-memory-utilization 0.4 \n",
      "\n",
      "export CUDA_VISIBLE_DEVICES=0\n",
      "python -m vllm.entrypoints.openai.api_server --model Rakuten/RakutenAI-7B-chat --max-model-len 4000 --port 8002 --gpu-memory-utilization 0.2 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conf_path=\"config.yaml\"\n",
    "\n",
    "with open(conf_path,\"r\") as f:\n",
    "    conf=yaml.safe_load(f.read())\n",
    "\n",
    "\n",
    "print(launch_command(conf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hatakeyama-llm-team/Tanuki-8B-Instruct': {'client': <openai.OpenAI at 0x7f74edfc3f10>,\n",
       "  'config': {'name': 'hatakeyama-llm-team/Tanuki-8B-Instruct',\n",
       "   'PORT': 8000,\n",
       "   'max-model-len': 4000,\n",
       "   'gpu-memory-utilization': 0.2,\n",
       "   'GPU_ID': 1,\n",
       "   'template1': '以下は、タスクを説明する指示と、文脈のある入力の組み合わせです。要求を適切に満たす応答を書きなさい。\\n\\n### 指示:\\n',\n",
       "   'template2': '\\n\\n### 応答:\\n',\n",
       "   'max_tokens': 2000}},\n",
       " 'microsoft/Phi-3-medium-128k-instruct': {'client': <openai.OpenAI at 0x7f74edfd9e90>,\n",
       "  'config': {'name': 'microsoft/Phi-3-medium-128k-instruct',\n",
       "   'PORT': 8001,\n",
       "   'max-model-len': 4000,\n",
       "   'gpu-memory-utilization': 0.4,\n",
       "   'GPU_ID': 1,\n",
       "   'template1': '<|user|>\\n',\n",
       "   'template2': '<|end|>\\n<|assistant|>',\n",
       "   'max_tokens': 2000}},\n",
       " 'Rakuten/RakutenAI-7B-chat': {'client': <openai.OpenAI at 0x7f74edfe5590>,\n",
       "  'config': {'name': 'Rakuten/RakutenAI-7B-chat',\n",
       "   'PORT': 8002,\n",
       "   'model': 'Rakuten/RakutenAI-7B-chat',\n",
       "   'max-model-len': 4000,\n",
       "   'gpu-memory-utilization': 0.2,\n",
       "   'max_tokens': 2000,\n",
       "   'GPU_ID': 0,\n",
       "   'template1': \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: ASSISTANT:\",\n",
       "   'template2': 'ASSISTANT:'}}}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client_dict=get_client_dict(conf)\n",
    "client_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' いいえ、ほどよい感じです。\\n元気な時、普通な時、元気でない時、いろいろな状態がありますが、認めたくないことは隠し、素直な状態を受け入れていきます。\\nそうすることで、苦しい時には助けを求めに患者に、上手に時には患者様の立場にたって仕事をすることが出来るのではないでしょうか。\\n少しでも患者にとってより良くなるようにできる看護師、そういう看護師になりパン職員として、助けを必要としている人を助けるためのラブある看護を提供したいと考えています。\\nそれでは、患者さんのために一所懸命に努力して、それぞれのよい結果を得られるよう努力していきましょう。よろしくお願いします。'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name=\"hatakeyama-llm-team/Tanuki-8B-Instruct\"\n",
    "model_name=\"microsoft/Phi-3-medium-128k-instruct\"\n",
    "model_name=\"Rakuten/RakutenAI-7B-chat\"\n",
    "question=\"元気ですか?\"\n",
    "res=ask_llm_prompt(client_dict,model_name,question)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Completion(id='cmpl-0d2ade79a5df4a51a11612391f123c2b', choices=[CompletionChoice(finish_reason='length', index=0, logprobs=None, text=' はい、元気です。あなたはどうですか？何かお困りのこと', stop_reason=None)], created=1717579730, model='hatakeyama-llm-team/Tanuki-8B-Instruct', object='text_completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=16, prompt_tokens=55, total_tokens=71))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "template1=\"以下は、タスクを説明する指示と、文脈のある入力の組み合わせです。要求を適切に満たす応答を書きなさい。\\n\\n### 指示:\\n\"\n",
    "template2=\"\\n\\n### 応答:\\n\"\n",
    "prompt=template1+question+template2\n",
    "completion = client.completions.create(model=model,\n",
    "                                      prompt=prompt)\n",
    "                                      \n",
    "completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "あなたは優秀な編集者です。次の文章を校正して下さい。「イタレーション速度が当初想定していたよりも進みが遅い」\n",
      "no question to answer\n",
      "no question to answer\n",
      "no question to answer\n",
      "no question to answer\n",
      "no question to answer\n",
      "no question to answer\n",
      "no question to answer\n",
      "no question to answer\n",
      "no question to answer\n",
      "no question to answer\n",
      "no question to answer\n",
      "no question to answer\n",
      "no question to answer\n",
      "no question to answer\n",
      "no question to answer\n",
      "今は何時ですか\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "import datetime\n",
    "from src.GeneralBot import GeneralBot\n",
    "from src.TanukiBot import TanukiBot\n",
    "from src.Client import Client\n",
    "\n",
    "# %%\n",
    "\n",
    "with open(\"env/url.txt\") as f:\n",
    "    url = f.read().strip()\n",
    "\n",
    "# apiクライアントとchatbotを起動\n",
    "client = Client(url)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "model_list = list(client_dict.keys())\n",
    "\n",
    "\n",
    "# %%\n",
    "while True:\n",
    "    row_id, question, inst = client.get_unanswered_question()\n",
    "    if question == \"\":\n",
    "        print(\"no question to answer\")\n",
    "        time.sleep(10)\n",
    "        continue\n",
    "\n",
    "    print(question)\n",
    "    model_name_A, model_name_B = random.sample(model_list, 2)\n",
    "    responseA=ask_llm(client_dict,model_name_A,question)\n",
    "    responseB=ask_llm(client_dict,model_name_B,question)\n",
    "    meta1 = client_dict[model_name_A][\"model\"]\n",
    "    meta2 = client_dict[model_name_B][\"model\"]\n",
    "    meta3 = datetime.datetime.now().isoformat()\n",
    "\n",
    "    client.answer(row_id, responseA, responseB, metainfo1=meta1,\n",
    "                  metainfo2=meta2, metainfo3=meta3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmeval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
