{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import csv\n",
    "from glicko2 import Player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path='data/0825.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# モデルのレーティングを保持する辞書\n",
    "models = {}\n",
    "\n",
    "# レーティング変更履歴を保存するリスト\n",
    "rating_history = []\n",
    "\n",
    "def get_or_create_player(model_name):\n",
    "    if model_name not in models:\n",
    "        models[model_name] = Player(rating=1500, rd=350, vol=0.06)\n",
    "    return models[model_name]\n",
    "\n",
    "def update_ratings(model1, model2, result, index):\n",
    "    player1 = get_or_create_player(model1)\n",
    "    player2 = get_or_create_player(model2)\n",
    "    \n",
    "    # 更新前のレーティングとRDを保存\n",
    "    rating1, rd1 = player1.rating, player1.rd\n",
    "    rating2, rd2 = player2.rating, player2.rd\n",
    "    \n",
    "    # result: 1 = model1の勝利, 0.5 = 引き分け, 0 = model2の勝利\n",
    "    player1.update_player([rating2], [rd2], [result])\n",
    "    player2.update_player([rating1], [rd1], [1 - result])\n",
    "    \n",
    "    # レーティング変更履歴を保存\n",
    "    rating_history.append({\n",
    "        'index': index,\n",
    "        'model1': model1,\n",
    "        'model2': model2,\n",
    "        'rating1_before': rating1,\n",
    "        'rating1_after': player1.rating,\n",
    "        'rating2_before': rating2,\n",
    "        'rating2_after': player2.rating,\n",
    "        'result': result\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#モデルの対戦回数と勝率\n",
    "total_battle_count = {}\n",
    "win_count = {}\n",
    "total_battles=0\n",
    "# CSVファイルを読み込む\n",
    "with open(csv_path, 'r', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file)\n",
    "    next(reader)  # ヘッダーをスキップ\n",
    "    \n",
    "    for index, row in enumerate(reader, start=1):\n",
    "        try:\n",
    "            evaluation = int(row[3])\n",
    "        except:\n",
    "            continue  # 評価が空白のものはスキップ\n",
    "        model1 = row[5]\n",
    "        model2 = row[6]\n",
    "        total_battles+=1\n",
    "        \n",
    "        if evaluation == 1:\n",
    "            update_ratings(model1, model2, 1, index)\n",
    "        elif evaluation == 2:\n",
    "            update_ratings(model1, model2, 0, index)\n",
    "        elif evaluation == 3:  # 両方良い場合（引き分け）\n",
    "            update_ratings(model1, model2, 0.5, index)\n",
    "        elif evaluation == 0:  # 両方良い場合（引き分け）\n",
    "            update_ratings(model1, model2, 0.5, index)\n",
    "\n",
    "\n",
    "        # evaluation が 0 の場合は更新しない（両方悪い場合）\n",
    "\n",
    "        # モデルの対戦回数と勝率を更新\n",
    "        if model1 not in total_battle_count:\n",
    "            total_battle_count[model1] = 0\n",
    "            win_count[model1] = 0\n",
    "        if model2 not in total_battle_count:\n",
    "            total_battle_count[model2] = 0\n",
    "            win_count[model2] = 0\n",
    "        total_battle_count[model1] += 1\n",
    "        total_battle_count[model2] += 1\n",
    "        if evaluation == 1:\n",
    "            win_count[model1] += 1\n",
    "        elif evaluation == 2:\n",
    "            win_count[model2] += 1\n",
    "\n",
    "total_battle_count,win_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(total_battle_count.values())/2,total_battles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#boundには1σの値を入れる\n",
    "sigma_val=1\n",
    "\n",
    "# 結果をレーティング順にソートして表示\n",
    "print(\"Model Ratings (sorted by rating):\")\n",
    "sorted_models = sorted(models.items(), key=lambda x: x[1].rating, reverse=True)\n",
    "\n",
    "def rename_model(model):\n",
    "    if model.find(\"/\")!=-1:\n",
    "        model_=model.split(\"/\")[1]\n",
    "    else:\n",
    "        model_=model\n",
    "    if model.find(\"plamo\")!= -1:\n",
    "        model_=\"PLAMO-100B\"\n",
    "    model_=model_.replace(\"gpt\",\"GPT\")\n",
    "    model_=model_.replace(\"3-5\",\"3.5\")\n",
    "\n",
    "    model_=model_[0].upper()+model_[1:]\n",
    "    return model_\n",
    "\n",
    "sorted_data=[]\n",
    "for model, player in sorted_models:\n",
    "    rating = player.rating\n",
    "    rd = player.rd\n",
    "    lower_bound = rating - sigma_val * rd\n",
    "    upper_bound = rating + sigma_val * rd\n",
    "    print(f\"{model}: Rating = {rating:.2f}, RD = {rd:.2f} (95% CI: {lower_bound:.2f} - {upper_bound:.2f})\")\n",
    "    \n",
    "    #先頭文字は大文字\n",
    "    model_=rename_model(model)\n",
    "    #勝率\n",
    "    win_rate=win_count[model]/total_battle_count[model]\n",
    "    d={\"name\":model_,\"name_original\":model,\n",
    "       \"win_rate\":win_rate,\n",
    "       \"total_battle_count\":total_battle_count[model],\n",
    "       \"rating\":rating,\"rd\":rd,\n",
    "       \"lower_bound\":lower_bound,\"upper_bound\":upper_bound,\n",
    "\n",
    "       }\n",
    "      \n",
    "       \n",
    "    sorted_data.append(d)\n",
    "\n",
    "\"\"\"\n",
    "# レーティング変更履歴をCSVファイルに保存\n",
    "with open('data/rating_history.csv', 'w', newline='', encoding='utf-8') as file:\n",
    "    writer = csv.DictWriter(file, fieldnames=[\n",
    "        'index', 'model1', 'model2', 'rating1_before', 'rating1_after',\n",
    "        'rating2_before', 'rating2_after', 'result'\n",
    "    ])\n",
    "    writer.writeheader()\n",
    "    writer.writerows(rating_history)\n",
    "\n",
    "print(\"Rating history has been saved to 'rating_history.csv'\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2sigmaでのグラフ\n",
    "df=pd.DataFrame(sorted_data)\n",
    "plt.figure(figsize=(8,5),dpi=150)\n",
    "sns.barplot(x=\"rating\",y=\"name\",data=df)\n",
    "#エラーバーを表示. プロットは打たない\n",
    "plt.errorbar(df[\"rating\"],df[\"name\"],xerr=df[\"rd\"]*2,fmt='None',color='black',capsize=5,elinewidth=1,alpha=0.5)\n",
    "plt.xlim(1100,1900)\n",
    "plt.xlabel(\"Glicko-2 Rating\")\n",
    "plt.ylabel(\"Model name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1sigmaでのグラフ\n",
    "df=pd.DataFrame(sorted_data)\n",
    "plt.figure(figsize=(8,5),dpi=150)\n",
    "sns.barplot(x=\"rating\",y=\"name\",data=df)\n",
    "#エラーバーを表示. プロットは打たない\n",
    "plt.errorbar(df[\"rating\"],df[\"name\"],xerr=df[\"rd\"],fmt='None',color='black',capsize=5,elinewidth=1,alpha=0.5)\n",
    "plt.xlim(1100,1800)\n",
    "plt.xlabel(\"Glicko-2 Rating\")\n",
    "plt.ylabel(\"Model name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#信頼区間での順位を予測\n",
    "import copy\n",
    "\n",
    "df=pd.DataFrame(sorted_data)\n",
    "df[\"lowerst_rank\"]=0\n",
    "df[\"highest_rank\"]=0\n",
    "model_list=df[\"name\"].tolist()\n",
    "\n",
    "\n",
    "for model in model_list:\n",
    "    df_lowest=copy.deepcopy(df)\n",
    "    #当該モデルにとって最も悲観的なレーティングを設定\n",
    "    df_lowest[\"rating\"]=df_lowest[\"upper_bound\"]\n",
    "    df_lowest.loc[df_lowest[\"name\"] == model, \"rating\"] = df_lowest.loc[df_lowest[\"name\"] == model, \"lower_bound\"]\n",
    "\n",
    "    #レーティング順にソート\n",
    "    df_lowest=df_lowest.sort_values(\"rating\",ascending=False).reset_index(drop=True)\n",
    "    \n",
    "    #当該モデルの順位を取得\n",
    "    rank=df_lowest[df_lowest[\"name\"]==model].index[0]+1\n",
    "    df.loc[df[\"name\"]==model,\"lowerst_rank\"]=rank\n",
    "\n",
    "    #当該モデルにとって最も楽観的なレーティングを設定\n",
    "    df_highest=copy.deepcopy(df)\n",
    "    df_highest[\"rating\"]=df_highest[\"lower_bound\"]\n",
    "    df_highest.loc[df_highest[\"name\"] == model, \"rating\"] = df_highest.loc[df_highest[\"name\"] == model, \"upper_bound\"]\n",
    "\n",
    "    #レーティング順にソート\n",
    "    df_highest=df_highest.sort_values(\"rating\",ascending=False).reset_index(drop=True)\n",
    "\n",
    "    #当該モデルの順位を取得\n",
    "    rank=df_highest[df_highest[\"name\"]==model].index[0]+1\n",
    "    df.loc[df[\"name\"]==model,\"highest_rank\"]=rank\n",
    "\n",
    "\n",
    "df[\"rank_range\"]=df[\"highest_rank\"].astype(str)+\"-\"+df[\"lowerst_rank\"].astype(str)\n",
    "df[\"rating\"]=df[\"rating\"].astype(int)\n",
    "df[\"lower_bound\"]=df[\"lower_bound\"].astype(int)\n",
    "df[\"upper_bound\"]=df[\"upper_bound\"].astype(int)\n",
    "df[\"rd\"]=df[\"rd\"].astype(int)\n",
    "df[\"win_rate\"]=df[\"win_rate\"].map(lambda x:round(x,2))\n",
    "df=df.drop([\"lowerst_rank\",\"highest_rank\",\"name_original\",\"lower_bound\",\"upper_bound\"],axis=1)\n",
    "\n",
    "\n",
    "#leaderboard3の結果も反映\n",
    "ld3_score_dict={\n",
    "'Claude-3.5-sonnet-20240620':0.82,\n",
    " 'GPT-4o-2024-05-13':0.78,\n",
    " 'Tanuki-8x8B-dpo-v1.0':0.57,\n",
    " 'Gemini-1.5-pro':0.73,\n",
    " 'Gemini-1.5-flash':0.70,\n",
    " 'GPT-4o-mini-2024-07-18':0.72,\n",
    " 'Calm3-22b-chat':0.65,\n",
    " 'Tanuki-8B-dpo-v1.0':0.52,\n",
    " 'PLAMO-100B':-1,\n",
    " 'Llama-3-ELYZA-JP-8B':0.62,\n",
    " 'Karakuri-lm-8x7b-chat-v0.1':0.6,\n",
    " 'GPT-3.5-turbo':0.58,\n",
    " 'Llama-3-Swallow-70B-Instruct-v0.1':0.65,\n",
    "}\n",
    "df[\"LD3-ave\"]=df[\"name\"].map(ld3_score_dict)\n",
    "\n",
    "ld3_jaster_zero_shot_score_dict={\n",
    "'Claude-3.5-sonnet-20240620':0.7,\n",
    " 'GPT-4o-2024-05-13':0.69,\n",
    " 'Tanuki-8x8B-dpo-v1.0':0.32,\n",
    " 'Gemini-1.5-pro':0.59,\n",
    " 'Gemini-1.5-flash':0.57,\n",
    " 'GPT-4o-mini-2024-07-18':0.64,\n",
    " 'Calm3-22b-chat':0.57,\n",
    " 'Tanuki-8B-dpo-v1.0':0.22,\n",
    " 'PLAMO-100B':-1,\n",
    " 'Llama-3-ELYZA-JP-8B':0.42,\n",
    " 'Karakuri-lm-8x7b-chat-v0.1':0.48,\n",
    " 'GPT-3.5-turbo':0.44,\n",
    " 'Llama-3-Swallow-70B-Instruct-v0.1':0.5,\n",
    "}\n",
    "df[\"LD3-Jaster-zero-shot\"]=df[\"name\"].map(ld3_jaster_zero_shot_score_dict)\n",
    "\n",
    "#ld3のmt benchも反映\n",
    "ld3_mt_score_dict={\n",
    "'Claude-3.5-sonnet-20240620':8.7,\n",
    " 'GPT-4o-2024-05-13':8.6,\n",
    " 'Tanuki-8x8B-dpo-v1.0':7.0,\n",
    " 'Gemini-1.5-pro':7.9,\n",
    " 'Gemini-1.5-flash':7.6,\n",
    " 'GPT-4o-mini-2024-07-18':8.3,\n",
    " 'Calm3-22b-chat':6.9,\n",
    " 'Tanuki-8B-dpo-v1.0':6.6,\n",
    " 'PLAMO-100B':-1,\n",
    " 'Llama-3-ELYZA-JP-8B':6.1,\n",
    " 'Karakuri-lm-8x7b-chat-v0.1':5.8,\n",
    " 'GPT-3.5-turbo':6.8,\n",
    " 'Llama-3-Swallow-70B-Instruct-v0.1':6.2,\n",
    "}\n",
    "df[\"JMT-Bench\"]=df[\"name\"].map(ld3_mt_score_dict)\n",
    "\n",
    "df=df.sort_values(\"win_rate\",ascending=False).reset_index(drop=True)\n",
    "df.to_csv(\"data/leaderboard.csv\",index=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#メモリを内向き\n",
    "plt.figure(figsize=(4,4),dpi=150)\n",
    "plt.rcParams['xtick.direction'] = 'in'\n",
    "plt.rcParams['ytick.direction'] = 'in'\n",
    "\n",
    "sns.scatterplot(x=\"JMT-Bench\",y=\"win_rate\",data=df[df[\"JMT-Bench\"]!=-1])\n",
    "plt.xlabel(\"Japanese MT-Bench\")\n",
    "plt.ylabel(\"Win rate\")\n",
    "#内向きメモリ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#メモリを内向き\n",
    "plt.figure(figsize=(4,4),dpi=150)\n",
    "plt.rcParams['xtick.direction'] = 'in'\n",
    "plt.rcParams['ytick.direction'] = 'in'\n",
    "\n",
    "sns.scatterplot(x=\"LD3-Jaster-zero-shot\",y=\"rating\",data=df[df[\"LD3-Jaster-zero-shot\"]!=-1])\n",
    "plt.xlabel(\"Jaster-zero-shot\")\n",
    "#plt.ylabel(\"Win rate\")\n",
    "#内向きメモリ\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#メモリを内向き\n",
    "plt.figure(figsize=(4,4),dpi=150)\n",
    "plt.rcParams['xtick.direction'] = 'in'\n",
    "plt.rcParams['ytick.direction'] = 'in'\n",
    "\n",
    "sns.scatterplot(x=\"JMT-Bench\",y=\"rating\",data=df[df[\"JMT-Bench\"]!=-1])\n",
    "#yはエラーバーも表示, rdの2倍\n",
    "sel_df=df[df[\"JMT-Bench\"]!=-1]\n",
    "plt.errorbar(sel_df[\"JMT-Bench\"],sel_df[\"rating\"],yerr=sel_df[\"rd\"],\n",
    "             fmt='None',color='gray',capsize=5,elinewidth=1,alpha=0.5)\n",
    "plt.xlabel(\"Japanese MT-Bench\")\n",
    "plt.ylabel(\"Rating\")\n",
    "#内向きメモリ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#メモリを内向き\n",
    "plt.figure(figsize=(4,4),dpi=150)\n",
    "plt.rcParams['xtick.direction'] = 'in'\n",
    "plt.rcParams['ytick.direction'] = 'in'\n",
    "\n",
    "sns.scatterplot(x=\"LD3-ave\",y=\"rating\",data=df[df[\"JMT-Bench\"]!=-1])\n",
    "plt.xlabel(\"Leaderboard3 average score\")\n",
    "#plt.ylabel(\"Win rate\")\n",
    "#内向きメモリ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#相関係数\n",
    "sel_df=df[df[\"JMT-Bench\"]!=-1]\n",
    "sel_df[[\"win_rate\",\"rating\",\n",
    "        \"JMT-Bench\",\"LD3-Jaster-zero-shot\",]].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#メモリを内向き\n",
    "plt.figure(figsize=(4,4),dpi=150)\n",
    "plt.rcParams['xtick.direction'] = 'in'\n",
    "plt.rcParams['ytick.direction'] = 'in'\n",
    "\n",
    "sns.scatterplot(x=\"LD3-Jaster-zero-shot\",y=\"JMT-Bench\",data=df[df[\"JMT-Bench\"]!=-1])\n",
    "plt.xlabel(\"LD3-Jaster-zero-shot\")\n",
    "plt.ylabel(\"JMT-Bench\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#メモリを内向き\n",
    "plt.figure(figsize=(4,4),dpi=150)\n",
    "plt.rcParams['xtick.direction'] = 'in'\n",
    "plt.rcParams['ytick.direction'] = 'in'\n",
    "plt.errorbar(df[\"win_rate\"],df[\"rating\"],yerr=df[\"rd\"],\n",
    "             fmt='None',color='gray',capsize=5,elinewidth=1,alpha=0.5)\n",
    "#sns.scatterplot(x=\"win_rate\",y=\"rating\",data=df)\n",
    "\n",
    "#回帰直線をseabornで引く\n",
    "sns.regplot(x=\"win_rate\",y=\"rating\",data=df,ci=0,line_kws={\"alpha\":0.4})\n",
    "\n",
    "plt.xlabel(\"win_rate\")\n",
    "plt.ylabel(\"rating\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chatbot arena本家でのrating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import plotly.express as px\n",
    "\n",
    "def compute_mle_elo(df, SCALE=400, BASE=10, INIT_RATING=1000):\n",
    "    ptbl_a_win = pd.pivot_table(\n",
    "        df[df[\"winner\"] == \"model_a\"],\n",
    "        index=\"model_a\",\n",
    "        columns=\"model_b\",\n",
    "        aggfunc=\"size\",\n",
    "        fill_value=0,\n",
    "    )\n",
    "    \n",
    "    if sum(df[\"winner\"].isin([\"tie\", \"tie (bothbad)\"])) == 0:\n",
    "        ptbl_tie = pd.DataFrame(0, index=ptbl_a_win.index, columns=ptbl_a_win.columns)\n",
    "    else:\n",
    "        ptbl_tie = pd.pivot_table(\n",
    "            df[df[\"winner\"].isin([\"tie\", \"tie (bothbad)\"])],\n",
    "            index=\"model_a\",\n",
    "            columns=\"model_b\",\n",
    "            aggfunc=\"size\",\n",
    "            fill_value=0,\n",
    "        )\n",
    "        ptbl_tie = ptbl_tie + ptbl_tie.T\n",
    "\n",
    "    ptbl_b_win = pd.pivot_table(\n",
    "        df[df[\"winner\"] == \"model_b\"],\n",
    "        index=\"model_a\",\n",
    "        columns=\"model_b\",\n",
    "        aggfunc=\"size\",\n",
    "        fill_value=0,\n",
    "    )\n",
    "    ptbl_win = ptbl_a_win * 2 + ptbl_b_win.T * 2 + ptbl_tie\n",
    "\n",
    "    models = pd.Series(np.arange(len(ptbl_win.index)), index=ptbl_win.index)\n",
    "\n",
    "    p = len(models)\n",
    "    X = np.zeros([p * (p - 1) * 2, p])\n",
    "    Y = np.zeros(p * (p - 1) * 2)\n",
    "\n",
    "    cur_row = 0\n",
    "    sample_weights = []\n",
    "    for m_a in ptbl_win.index:\n",
    "        for m_b in ptbl_win.columns:\n",
    "            if m_a == m_b:\n",
    "                continue\n",
    "            if math.isnan(ptbl_win.loc[m_a, m_b]) or math.isnan(ptbl_win.loc[m_b, m_a]):\n",
    "                continue\n",
    "            X[cur_row, models[m_a]] = +math.log(BASE)\n",
    "            X[cur_row, models[m_b]] = -math.log(BASE)\n",
    "            Y[cur_row] = 1.0\n",
    "            sample_weights.append(ptbl_win.loc[m_a, m_b])\n",
    "\n",
    "            X[cur_row + 1, models[m_a]] = math.log(BASE)\n",
    "            X[cur_row + 1, models[m_b]] = -math.log(BASE)\n",
    "            Y[cur_row + 1] = 0.0\n",
    "            sample_weights.append(ptbl_win.loc[m_b, m_a])\n",
    "            cur_row += 2\n",
    "    X = X[:cur_row]\n",
    "    Y = Y[:cur_row]\n",
    "\n",
    "    lr = LogisticRegression(fit_intercept=False, penalty=None, tol=1e-6)\n",
    "    lr.fit(X, Y, sample_weight=sample_weights)\n",
    "    elo_scores = SCALE * lr.coef_[0] + INIT_RATING\n",
    "    if \"weblab-GENIAC/Tanuki-8B-dpo-v1.0\" in models.index:\n",
    "        elo_scores += 1000 - elo_scores[models[\"weblab-GENIAC/Tanuki-8B-dpo-v1.0\"]]\n",
    "    return pd.Series(elo_scores, index=models.index).sort_values(ascending=False)\n",
    "\n",
    "def get_bootstrap_result(battles, func_compute_elo, num_round):\n",
    "    rows = []\n",
    "    for i in tqdm(range(num_round), desc=\"bootstrap\"):\n",
    "        rows.append(func_compute_elo(battles.sample(frac=1.0, replace=True)))\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df[df.median().sort_values(ascending=False).index]\n",
    "\n",
    "def visualize_bootstrap_scores(df, title):\n",
    "    bars = pd.DataFrame(dict(\n",
    "        lower = df.quantile(.025),\n",
    "        rating = df.quantile(.5),\n",
    "        upper = df.quantile(.975))).reset_index(names=\"model\").sort_values(\"rating\", ascending=False)\n",
    "    bars['error_y'] = bars['upper'] - bars[\"rating\"]\n",
    "    bars['error_y_minus'] = bars['rating'] - bars[\"lower\"]\n",
    "    bars['rating_rounded'] = np.round(bars['rating'], 2)\n",
    "    bars[\"model_name\"]=bars[\"model\"].map(rename_model)\n",
    "    fig = px.scatter(bars, x=\"model_name\", y=\"rating\", error_y=\"error_y\",\n",
    "                     error_y_minus=\"error_y_minus\", text=\"rating_rounded\",\n",
    "                     title=title)\n",
    "    fig.update_layout(xaxis_title=\"Model\", yaxis_title=\"Rating\",\n",
    "                      height=600)\n",
    "    return fig\n",
    "\n",
    "# CSVファイルを読み込む\n",
    "battles = []\n",
    "with open(csv_path, 'r', encoding='utf-8') as file:\n",
    "    reader = csv.reader(file)\n",
    "    next(reader)  # ヘッダーをスキップ\n",
    "    \n",
    "    for index, row in enumerate(reader, start=1):\n",
    "        try:\n",
    "            evaluation = int(row[3])\n",
    "        except:\n",
    "            continue  # 評価が空白のものはスキップ\n",
    "        model_a = row[5]\n",
    "        model_b = row[6]\n",
    "        \n",
    "        if evaluation == 1:\n",
    "            winner = \"model_a\"\n",
    "        elif evaluation == 2:\n",
    "            winner = \"model_b\"\n",
    "        elif evaluation == 3:  # 両方良い場合（引き分け）\n",
    "            winner = \"tie\"\n",
    "        elif evaluation == 0:  # 両方悪い場合（引き分け）\n",
    "            winner = \"tie (bothbad)\"\n",
    "        \n",
    "        battles.append({\n",
    "            \"model_a\": model_a,\n",
    "            \"model_b\": model_b,\n",
    "            \"winner\": winner\n",
    "        })\n",
    "\n",
    "# DataFrameに変換\n",
    "df_battles = pd.DataFrame(battles)\n",
    "\n",
    "# MLEを使用してEloレーティングを計算\n",
    "elo_mle_ratings = compute_mle_elo(df_battles)\n",
    "\n",
    "# 結果を表示\n",
    "print(\"Model Ratings (sorted by MLE Elo rating):\")\n",
    "for i, (model, rating) in enumerate(elo_mle_ratings.items(), start=1):\n",
    "    print(f\"{i}. {model}: MLE Elo Rating = {rating:.2f}\")\n",
    "\n",
    "# レーティングをCSVファイルに保存\n",
    "elo_mle_ratings.to_csv('mle_elo_ratings.csv', header=True)\n",
    "print(\"MLE Elo ratings have been saved to 'mle_elo_ratings.csv'\")\n",
    "\n",
    "# ブートストラップ法を使用して信頼区間を計算\n",
    "BOOTSTRAP_ROUNDS = 100\n",
    "np.random.seed(42)\n",
    "bootstrap_elo_lu = get_bootstrap_result(df_battles, compute_mle_elo, BOOTSTRAP_ROUNDS)\n",
    "\n",
    "# 結果を可視化\n",
    "fig = visualize_bootstrap_scores(bootstrap_elo_lu, \"Bootstrap of MLE Elo Rating Estimates\")\n",
    "fig.show()\n",
    "\n",
    "# 信頼区間を含めた結果を表示\n",
    "confidence_intervals = pd.DataFrame({\n",
    "    'Lower CI': bootstrap_elo_lu.quantile(0.025),\n",
    "    'Median': bootstrap_elo_lu.quantile(0.5),\n",
    "    'Upper CI': bootstrap_elo_lu.quantile(0.975)\n",
    "}).sort_values('Median', ascending=False)\n",
    "\n",
    "elo_dict=elo_mle_ratings.to_dict()\n",
    "\n",
    "print(\"\\nModel Ratings with Confidence Intervals:\")\n",
    "for model in confidence_intervals.index:\n",
    "    lower = confidence_intervals.loc[model, 'Lower CI']\n",
    "    median = confidence_intervals.loc[model, 'Median']\n",
    "    upper = confidence_intervals.loc[model, 'Upper CI']\n",
    "    print(f\"{model}: Median = {median:.2f}, 95% CI = [{lower:.2f}, {upper:.2f}]\")\n",
    "\n",
    "# 結果をCSVファイルに保存\n",
    "confidence_intervals.to_csv('mle_elo_ratings_with_ci.csv', header=True)\n",
    "print(\"MLE Elo ratings with confidence intervals have been saved to 'mle_elo_ratings_with_ci.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elo_dict_={rename_model(k):v for k,v in elo_dict.items()}\n",
    "\n",
    "#dfに追加\n",
    "show_df=copy.deepcopy(df)\n",
    "show_df[\"Rating\"]=show_df[\"name\"].map(elo_dict_).astype(int)\n",
    "show_df=show_df.drop([\"rd\",\"rank_range\",],axis=1)\n",
    "show_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_df[show_df[\"JMT-Bench\"]!=-1].drop([\"name\",\"total_battle_count\"],axis=1).corr()\n",
    "#show_df.drop([\"name\",\"total_battle_count\"],axis=1).corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elo_dict\n",
    "glicko_dict={k:models[k].rating for k in models.keys()}\n",
    "\n",
    "#EloとGlickoの比較\n",
    "compare_df=pd.DataFrame({\"Elo\":elo_dict,\"Glicko\":glicko_dict})\n",
    "compare_df.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(x=\"Elo\",y=\"Glicko\",data=compare_df,ci=0)\n",
    "plt.xlabel(\"Bradley-Terry\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmeval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
