{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from src.VLLMServer import launch_command,get_client_dict,ask_llm,ask_llm_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export CUDA_VISIBLE_DEVICES=0\n",
      "python -m vllm.entrypoints.openai.api_server --model hatakeyama-llm-team/Tanuki-8B-Instruct --max-model-len 4000 --port 8000 --gpu-memory-utilization 0.2 \n",
      "\n",
      "export CUDA_VISIBLE_DEVICES=1\n",
      "python -m vllm.entrypoints.openai.api_server --model microsoft/Phi-3-medium-128k-instruct --max-model-len 4000 --port 8001 --gpu-memory-utilization 0.4 \n",
      "\n",
      "export CUDA_VISIBLE_DEVICES=2\n",
      "python -m vllm.entrypoints.openai.api_server --model Rakuten/RakutenAI-7B-chat --max-model-len 4000 --port 8002 --gpu-memory-utilization 0.2 \n",
      "\n",
      "export CUDA_VISIBLE_DEVICES=3,4\n",
      "python -m vllm.entrypoints.openai.api_server --model karakuri-ai/karakuri-lm-8x7b-chat-v0.1 --max-model-len 4000 --port 8010 --tensor-parallel-size 2 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conf_path=\"config.yaml\"\n",
    "\n",
    "with open(conf_path,\"r\") as f:\n",
    "    conf=yaml.safe_load(f.read())\n",
    "\n",
    "\n",
    "print(launch_command(conf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hatakeyama-llm-team/Tanuki-8B-Instruct': {'client': <openai.OpenAI at 0x7fd33776c610>,\n",
       "  'config': {'name': 'hatakeyama-llm-team/Tanuki-8B-Instruct',\n",
       "   'PORT': 8000,\n",
       "   'max-model-len': 4000,\n",
       "   'gpu-memory-utilization': 0.2,\n",
       "   'GPU_ID': 0,\n",
       "   'template1': '以下は、タスクを説明する指示と、文脈のある入力の組み合わせです。要求を適切に満たす応答を書きなさい。\\n\\n### 指示:\\n',\n",
       "   'template2': '\\n\\n### 応答:\\n',\n",
       "   'max_tokens': 2000}},\n",
       " 'microsoft/Phi-3-medium-128k-instruct': {'client': <openai.OpenAI at 0x7fd33776f590>,\n",
       "  'config': {'name': 'microsoft/Phi-3-medium-128k-instruct',\n",
       "   'PORT': 8001,\n",
       "   'max-model-len': 4000,\n",
       "   'gpu-memory-utilization': 0.4,\n",
       "   'GPU_ID': 1,\n",
       "   'template1': '<|user|>\\n',\n",
       "   'template2': '<|end|>\\n<|assistant|>',\n",
       "   'max_tokens': 2000}},\n",
       " 'Rakuten/RakutenAI-7B-chat': {'client': <openai.OpenAI at 0x7fd337771c50>,\n",
       "  'config': {'name': 'Rakuten/RakutenAI-7B-chat',\n",
       "   'PORT': 8002,\n",
       "   'model': 'Rakuten/RakutenAI-7B-chat',\n",
       "   'max-model-len': 4000,\n",
       "   'gpu-memory-utilization': 0.2,\n",
       "   'max_tokens': 2000,\n",
       "   'GPU_ID': 2,\n",
       "   'template1': \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: ASSISTANT:\",\n",
       "   'template2': 'ASSISTANT:'}},\n",
       " 'karakuri-ai/karakuri-lm-8x7b-chat-v0.1': {'client': <openai.OpenAI at 0x7fd337781290>,\n",
       "  'config': {'name': 'karakuri-ai/karakuri-lm-8x7b-chat-v0.1',\n",
       "   'PORT': 8010,\n",
       "   'model': 'karakuri-ai/karakuri-lm-8x7b-chat-v0.1',\n",
       "   'max-model-len': 4000,\n",
       "   'max_tokens': 2000,\n",
       "   'GPU_ID': '3,4',\n",
       "   'tensor-parallel-size': 2,\n",
       "   'template1': '<s>[INST]',\n",
       "   'template2': '[/INST]'}}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client_dict=get_client_dict(conf)\n",
    "client_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'申し訳ありませんが、前提条件を満たしていないと思われます。純粋理性批判は哲学の書物であり、たぬきに理解させるのは困難が伴うと思われます。'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name=\"hatakeyama-llm-team/Tanuki-8B-Instruct\"\n",
    "model_name=\"microsoft/Phi-3-medium-128k-instruct\"\n",
    "model_name=\"Rakuten/RakutenAI-7B-chat\"\n",
    "model_name=\"karakuri-ai/karakuri-lm-8x7b-chat-v0.1\"\n",
    "\n",
    "question=\"純粋理性批判はたぬきに理解できますか?\"\n",
    "res=ask_llm_prompt(client_dict,model_name,question)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Completion(id='cmpl-0d2ade79a5df4a51a11612391f123c2b', choices=[CompletionChoice(finish_reason='length', index=0, logprobs=None, text=' はい、元気です。あなたはどうですか？何かお困りのこと', stop_reason=None)], created=1717579730, model='hatakeyama-llm-team/Tanuki-8B-Instruct', object='text_completion', system_fingerprint=None, usage=CompletionUsage(completion_tokens=16, prompt_tokens=55, total_tokens=71))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "template1=\"以下は、タスクを説明する指示と、文脈のある入力の組み合わせです。要求を適切に満たす応答を書きなさい。\\n\\n### 指示:\\n\"\n",
    "template2=\"\\n\\n### 応答:\\n\"\n",
    "prompt=template1+question+template2\n",
    "completion = client.completions.create(model=model,\n",
    "                                      prompt=prompt)\n",
    "                                      \n",
    "completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "あなたは優秀な編集者です。次の文章を校正して下さい。「イタレーション速度が当初想定していたよりも進みが遅い」\n",
      "no question to answer\n",
      "no question to answer\n",
      "no question to answer\n",
      "no question to answer\n",
      "no question to answer\n",
      "no question to answer\n",
      "no question to answer\n",
      "no question to answer\n",
      "no question to answer\n",
      "no question to answer\n",
      "no question to answer\n",
      "no question to answer\n",
      "no question to answer\n",
      "no question to answer\n",
      "no question to answer\n",
      "今は何時ですか\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "import datetime\n",
    "from src.GeneralBot import GeneralBot\n",
    "from src.TanukiBot import TanukiBot\n",
    "from src.Client import Client\n",
    "\n",
    "# %%\n",
    "\n",
    "with open(\"env/url.txt\") as f:\n",
    "    url = f.read().strip()\n",
    "\n",
    "# apiクライアントとchatbotを起動\n",
    "client = Client(url)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "model_list = list(client_dict.keys())\n",
    "\n",
    "\n",
    "# %%\n",
    "while True:\n",
    "    row_id, question, inst = client.get_unanswered_question()\n",
    "    if question == \"\":\n",
    "        print(\"no question to answer\")\n",
    "        time.sleep(10)\n",
    "        continue\n",
    "\n",
    "    print(question)\n",
    "    model_name_A, model_name_B = random.sample(model_list, 2)\n",
    "    responseA=ask_llm(client_dict,model_name_A,question)\n",
    "    responseB=ask_llm(client_dict,model_name_B,question)\n",
    "    meta1 = client_dict[model_name_A][\"model\"]\n",
    "    meta2 = client_dict[model_name_B][\"model\"]\n",
    "    meta3 = datetime.datetime.now().isoformat()\n",
    "\n",
    "    client.answer(row_id, responseA, responseB, metainfo1=meta1,\n",
    "                  metainfo2=meta2, metainfo3=meta3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmeval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
