{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from src.VLLMServer import launch_command,get_client_dict,ask_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export CUDA_VISIBLE_DEVICES=1\n",
      "python -m vllm.entrypoints.openai.api_server --model hatakeyama-llm-team/Tanuki-8B-Instruct --max-model-len 4000 --port 8000 --gpu-memory-utilization 0.2 \n",
      "export CUDA_VISIBLE_DEVICES=1\n",
      "python -m vllm.entrypoints.openai.api_server --model microsoft/Phi-3-medium-128k-instruct --max-model-len 4000 --port 8001 --gpu-memory-utilization 0.4 \n",
      "export CUDA_VISIBLE_DEVICES=0\n",
      "python -m vllm.entrypoints.openai.api_server --model Rakuten/RakutenAI-7B-chat --max-model-len 4000 --port 8002 --gpu-memory-utilization 0.2 --chat-template templates/RakutenAI-7B-chat.jinja\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conf_path=\"config.yaml\"\n",
    "\n",
    "with open(conf_path,\"r\") as f:\n",
    "    conf=yaml.safe_load(f.read())\n",
    "\n",
    "\n",
    "print(launch_command(conf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Tanuki-8B-Instruct': {'model': 'hatakeyama-llm-team/Tanuki-8B-Instruct',\n",
       "  'client': <openai.OpenAI at 0x7ff02e1f9990>},\n",
       " 'Phi-3-medium-128k-instruct': {'model': 'microsoft/Phi-3-medium-128k-instruct',\n",
       "  'client': <openai.OpenAI at 0x7fef172eab10>},\n",
       " 'RakutenAI-7B-chat': {'model': 'Rakuten/RakutenAI-7B-chat',\n",
       "  'client': <openai.OpenAI at 0x7fef17322d50>}}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client_dict=get_client_dict(conf)\n",
    "client_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "あなたは優秀な編集者です。次の文章を校正して下さい。「イタレーション速度が当初想定していたよりも進みが遅い」\n",
      "no question to answer\n",
      "no question to answer\n",
      "no question to answer\n",
      "no question to answer\n",
      "no question to answer\n",
      "no question to answer\n",
      "no question to answer\n",
      "no question to answer\n",
      "no question to answer\n",
      "no question to answer\n",
      "no question to answer\n",
      "no question to answer\n",
      "no question to answer\n",
      "no question to answer\n",
      "no question to answer\n",
      "今は何時ですか\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "import datetime\n",
    "from src.GeneralBot import GeneralBot\n",
    "from src.TanukiBot import TanukiBot\n",
    "from src.Client import Client\n",
    "\n",
    "# %%\n",
    "\n",
    "with open(\"env/url.txt\") as f:\n",
    "    url = f.read().strip()\n",
    "\n",
    "# apiクライアントとchatbotを起動\n",
    "client = Client(url)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# %%\n",
    "model_list = list(client_dict.keys())\n",
    "\n",
    "\n",
    "# %%\n",
    "while True:\n",
    "    row_id, question, inst = client.get_unanswered_question()\n",
    "    if question == \"\":\n",
    "        print(\"no question to answer\")\n",
    "        time.sleep(10)\n",
    "        continue\n",
    "\n",
    "    print(question)\n",
    "    model_name_A, model_name_B = random.sample(model_list, 2)\n",
    "    responseA=ask_llm(client_dict,model_name_A,question)\n",
    "    responseB=ask_llm(client_dict,model_name_B,question)\n",
    "    meta1 = client_dict[model_name_A][\"model\"]\n",
    "    meta2 = client_dict[model_name_B][\"model\"]\n",
    "    meta3 = datetime.datetime.now().isoformat()\n",
    "\n",
    "    client.answer(row_id, responseA, responseB, metainfo1=meta1,\n",
    "                  metainfo2=meta2, metainfo3=meta3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmeval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
