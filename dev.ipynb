{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from src.VLLMServer import launch_command,get_client_dict,ask_llm,ask_llm_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "export CUDA_VISIBLE_DEVICES=0\n",
      "python -m vllm.entrypoints.openai.api_server --model hatakeyama-llm-team/Tanuki-8B-Instruct --max-model-len 4000 --port 8000 --gpu-memory-utilization 0.2 \n",
      "\n",
      "export CUDA_VISIBLE_DEVICES=1\n",
      "python -m vllm.entrypoints.openai.api_server --model microsoft/Phi-3-medium-128k-instruct --max-model-len 4000 --port 8001 --gpu-memory-utilization 0.4 \n",
      "\n",
      "export CUDA_VISIBLE_DEVICES=2\n",
      "python -m vllm.entrypoints.openai.api_server --model Rakuten/RakutenAI-7B-chat --max-model-len 4000 --port 8002 --gpu-memory-utilization 0.2 \n",
      "\n",
      "export CUDA_VISIBLE_DEVICES=5\n",
      "python -m vllm.entrypoints.openai.api_server --model Qwen/Qwen2-7B-Instruct --max-model-len 4000 --port 8003 \n",
      "\n",
      "export CUDA_VISIBLE_DEVICES=3,4\n",
      "python -m vllm.entrypoints.openai.api_server --model karakuri-ai/karakuri-lm-8x7b-chat-v0.1 --max-model-len 4000 --port 8010 --tensor-parallel-size 2 \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "conf_path=\"config.yaml\"\n",
    "\n",
    "with open(conf_path,\"r\") as f:\n",
    "    conf=yaml.safe_load(f.read())\n",
    "\n",
    "\n",
    "print(launch_command(conf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hatakeyama-llm-team/Tanuki-8B-Instruct': {'client': <openai.OpenAI at 0x7f4be2aa89d0>,\n",
       "  'config': {'name': 'hatakeyama-llm-team/Tanuki-8B-Instruct',\n",
       "   'PORT': 8000,\n",
       "   'max-model-len': 4000,\n",
       "   'gpu-memory-utilization': 0.2,\n",
       "   'GPU_ID': 0,\n",
       "   'template1': '以下は、タスクを説明する指示と、文脈のある入力の組み合わせです。要求を適切に満たす応答を書きなさい。\\n\\n### 指示:\\n',\n",
       "   'template2': '\\n\\n### 応答:\\n',\n",
       "   'max_tokens': 2000}},\n",
       " 'microsoft/Phi-3-medium-128k-instruct': {'client': <openai.OpenAI at 0x7f4be2aabad0>,\n",
       "  'config': {'name': 'microsoft/Phi-3-medium-128k-instruct',\n",
       "   'PORT': 8001,\n",
       "   'max-model-len': 4000,\n",
       "   'gpu-memory-utilization': 0.4,\n",
       "   'GPU_ID': 1,\n",
       "   'template1': '<|user|>\\n',\n",
       "   'template2': '<|end|>\\n<|assistant|>',\n",
       "   'max_tokens': 2000}},\n",
       " 'Rakuten/RakutenAI-7B-chat': {'client': <openai.OpenAI at 0x7f4be2aadfd0>,\n",
       "  'config': {'name': 'Rakuten/RakutenAI-7B-chat',\n",
       "   'PORT': 8002,\n",
       "   'model': 'Rakuten/RakutenAI-7B-chat',\n",
       "   'max-model-len': 4000,\n",
       "   'gpu-memory-utilization': 0.2,\n",
       "   'max_tokens': 2000,\n",
       "   'GPU_ID': 2,\n",
       "   'template1': \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: ASSISTANT:\",\n",
       "   'template2': 'ASSISTANT:'}},\n",
       " 'Qwen/Qwen2-7B-Instruct': {'client': <openai.OpenAI at 0x7f4be2ac5b90>,\n",
       "  'config': {'name': 'Qwen/Qwen2-7B-Instruct',\n",
       "   'PORT': 8003,\n",
       "   'model': 'Qwen/Qwen2-7B-Instruct',\n",
       "   'max-model-len': 4000,\n",
       "   'max_tokens': 2000,\n",
       "   'GPU_ID': 5,\n",
       "   'template1': '<|im_start|>あなたは有用なアシスタントです。次の質問に日本語で回答してください。<|im_end|>\\n<|im_start|>',\n",
       "   'template2': '<|im_end|><|im_start|>assistant\\n'}},\n",
       " 'karakuri-ai/karakuri-lm-8x7b-chat-v0.1': {'client': <openai.OpenAI at 0x7f4be2ac9390>,\n",
       "  'config': {'name': 'karakuri-ai/karakuri-lm-8x7b-chat-v0.1',\n",
       "   'PORT': 8010,\n",
       "   'model': 'karakuri-ai/karakuri-lm-8x7b-chat-v0.1',\n",
       "   'max-model-len': 4000,\n",
       "   'max_tokens': 2000,\n",
       "   'GPU_ID': '3,4',\n",
       "   'tensor-parallel-size': 2,\n",
       "   'template1': '<s>[INST]',\n",
       "   'template2': '[/INST]'}}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "client_dict=get_client_dict(conf)\n",
    "client_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'「純粋理性批判」は、哲学の概念で、イマヌエル・カントによって提唱されました。この理論は、人間がどのように経験や認識を通じて世界を理解するか、理知的世界に影響を及ぼす概念や原則を別の観点から考え直すメタ論的アプローチです。\\n\\nたぬきという動物には人間が考え行動する规律や原理を理解する知識と経験がありません。したがって、たぬきに「純粋理性批判」という概念を理解することは不可能と言えるでしょう。しかし、たぬきが世界を理解するための自発的な行動や本能があったとしても、それらは一定の環境や遺伝的要素によって形成された本能として機能しています。これらは人間の理性と完全不同と考えられています。'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name=\"hatakeyama-llm-team/Tanuki-8B-Instruct\"\n",
    "model_name=\"microsoft/Phi-3-medium-128k-instruct\"\n",
    "model_name=\"Rakuten/RakutenAI-7B-chat\"\n",
    "model_name=\"karakuri-ai/karakuri-lm-8x7b-chat-v0.1\"\n",
    "model_name=\"Qwen/Qwen2-7B-Instruct\"\n",
    "\n",
    "question=\"純粋理性批判はたぬきに理解できますか?\"\n",
    "res=ask_llm_prompt(client_dict,model_name,question)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmeval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
