{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from src.VLLMServer import launch_command,get_client_dict,ask_llm,ask_llm_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_path=\"config.yaml\"\n",
    "\n",
    "with open(conf_path,\"r\") as f:\n",
    "    conf=yaml.safe_load(f.read())\n",
    "client_dict=get_client_dict(conf)\n",
    "#print(launch_command(conf))\n",
    "\n",
    "client_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name=\"hatakeyama-llm-team/Tanuki-8B-Instruct\"\n",
    "model_name=\"microsoft/Phi-3-medium-128k-instruct\"\n",
    "#model_name=\"Rakuten/RakutenAI-7B-chat\"\n",
    "#model_name=\"karakuri-ai/karakuri-lm-8x7b-chat-v0.1\"\n",
    "#model_name=\"Qwen/Qwen2-7B-Instruct\"\n",
    "#model_name=\"models--mistralai--Mixtral-8x22B-Instruct-v0.1\"\n",
    "model_name=\"llm-jp/llm-jp-13b-instruct-full-ac_001_16x-dolly-ichikara_004_001_single-oasst-oasst2-v2.0\"\n",
    "model_name=\"nvidia/Nemotron-4-340B-Instruct\"\n",
    "\n",
    "question=\"純粋理性批判はたぬきに理解できますか?\"\n",
    "#question=\"以下は、タスクを説明する指示です。要求を適切に満たす応答を書きなさい。### 指示:\\n\"\n",
    "res=ask_llm_prompt(client_dict,model_name,question)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt=\"以下は、タスクを説明する指示です。要求を適切に満たす応答を書きなさい。### 指示:\\n元気ですか\\n\\n### 応答:\\n\"\n",
    "client = client_dict[model_name][\"client\"]\n",
    "completion = client.completions.create(model=model_name,\n",
    "                                        prompt=prompt,\n",
    "                                        max_tokens=int(client_dict[model_name][\"config\"][\"max_tokens\"]))\n",
    "out=completion.choices[0].text.strip()  # .message.content.strip()\n",
    "out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmd=\"\"\"以下のテキストをもとに､日本語で教科書風の文章を生成しなさい｡\n",
    "-[問題文]を出力\n",
    "-[考え方]を出力\n",
    "-[答え]を出力\n",
    "-[詳細な解説]を出力\n",
    "\"\"\"\n",
    "#cmd=\"以下のテキストをもとに､複数の登場人物でなされる日本語の会話文を生成しなさい｡\"\n",
    "question=cmd+\"\"\"\n",
    "#テキスト\n",
    "Q.\n",
    "Rick took off on a road trip for the summer. He traveled to the first destination on his trip, and then from there, he traveled twice the distance to this second destination. The third point on his trip was 40 miles away, which was half the distance he traveled from the beginning to his first destination. The final destination took twice the amount of driving of all the other legs put together. How many miles in total did he travel across all the legs of the trip?\n",
    "\n",
    "A.\t\n",
    "Let's solve this problem using Python's sympy library. <llm-code> import sympy as sp # third leg was 40 miles x = 40 # third leg was half the first leg first_leg = 2 * x # second leg was twice the first leg second_leg = 2 * first_leg # fourth leg was equal to the sum of the three legs fourth_leg = x + first_leg + second_leg # adding up the four to get the total distance total_distance = x + first_leg + second_leg + fourth_leg total_distance </llm-code> <llm-code-output> 560 </llm-code-output> Thus Rick traveled \\boxed{560} miles in total.\n",
    "\n",
    "\"\"\"\n",
    "res=ask_llm_prompt(client_dict,model_name,question)\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = client_dict[model_name][\"client\"]\n",
    "\n",
    "template1 = client_dict[model_name][\"config\"][\"template1\"]\n",
    "template2 = client_dict[model_name][\"config\"][\"template2\"]\n",
    "prompt = template1+question+template2\n",
    "completion = client.completions.create(model=model_name,\n",
    "                                        prompt=prompt,\n",
    "                                        max_tokens=int(client_dict[model_name][\"config\"][\"max_tokens\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from datasets import load_dataset\n",
    "ds=load_dataset(\"tokyotech-llm/Swallow-Instruct-v0.1\",split=\"train\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_list=[]\n",
    "\n",
    "for record in ds:\n",
    "    for conv in record[\"conversation\"]:\n",
    "        if conv[\"role\"]==\"user\":\n",
    "            q_list.append(conv[\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame(q_list)\n",
    "df.to_csv(\"swallow.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmeval",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
